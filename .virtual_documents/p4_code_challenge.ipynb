





# Run this cell without changes
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, silhouette_score

import nltk
import re
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

from sklearn.cluster import KMeans








# Run this cell without changes to load in data
wheat_df = pd.read_csv('wheat_seeds.csv')
wheat_df.head()





# Run this cell without changing
wheat_df.info()


# Run this cell without changing
wheat_df.describe()




















# Run this cell without changes
steps = [('imp', SimpleImputer(strategy='mean')),
         ('scaler', StandardScaler()),
         ('knn', KNeighborsClassifier(n_neighbors=30))]
pipe = Pipeline(steps) 





# CodeGrade step1.1
# Replace None with appropriate code
# do the required data splitting here

# Assign X and y, use all columns but y for X
X = wheat_df.drop(columns="Type")
y = wheat_df["Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)





# Run this cell without changes
y.value_counts()





# CodeGrade step1.2
# Replace None with appropriate code

# Fit pipeline
pipe.fit(X_train, y_train)

# Test set predictions and accuracy score
y_pred = pipe.predict(X_test)
test_acc = accuracy_score(y_test, y_pred)
test_acc








# CodeGrade step1.3

# Setup grid for search
params = {
    'imp__strategy': ['mean', 'most_frequent'],
    'knn__n_neighbors': [1, 5, 10, 20, 30]
}

# Instanstiate grid search object
grid_knn = GridSearchCV(pipe, param_grid=params, 
                        scoring='accuracy', 
                        cv=5)

# Fit and get best model
grid_knn.fit(X_train, y_train)
best_pipe = grid_knn.best_estimator_





# Run this cell without changes
print(grid_knn.best_params_)





# CodeGrade step1.4
# Replace None with appropriate code

# Refit to train
best_pipe.fit(X_train, y_train)

# Test set predictions and scores
y_best_pred = best_pipe.predict(X_test)
tuned_test_acc = accuracy_score(y_test, y_best_pred)
tuned_test_acc























# Run this cell without changes to load in data
sentiment_data = pd.read_csv('sentiment_analysis.csv')
sentiment_data.head()














# Run this cell without changes to preprocess the text

def tokenize_and_preprocess(reviews):
    reviews = reviews.fillna("").astype(str)
    
    stop_words = stopwords.words('english')
    patt = re.compile(r'\b(' + r'|'.join(stop_words) + r')\b\s*')

    preproc_step1 = reviews.str.lower().str.replace(r'[0-9]+', '', regex=True).str.replace(patt, '', regex=True)
    
    # tokeniz. result is a Pandas series of document represented as lists of tokens
    preproc1_tokenized = preproc_step1.apply(word_tokenize)
    
    # inner function. takes in single document as token list.
    # processes further by stemming and removing non-alphabetic characters
    
    def remove_punct_and_stem(doc_tokenized):
        stemmer = SnowballStemmer('english')
        return " ".join([stemmer.stem(tok) for tok in doc_tokenized if tok.isalpha()])
    
    preprocessed = preproc1_tokenized.apply(remove_punct_and_stem)
    
    return preprocessed

sentiment_data['preprocessed_text'] = tokenize_and_preprocess(sentiment_data.reviewText)














# CodeGrade step3.1
# Replace None with appropriate code

X_sent = sentiment_data['preprocessed_text']
y_sent = sentiment_data['target']

X_sent_train, X_sent_test, y_sent_train, y_sent_test = train_test_split(
    X_sent, y_sent, test_size=0.3, random_state=42
)





# CodeGrade step3.2
# Replace None with appropriate code

nlp_pipe = Pipeline([
    ('tfidf', TfidfVectorizer(min_df=0.01, max_df=0.9)),
    ('nb', MultinomialNB())
])





# CodeGrade step3.3
# Replace None with appropriate code

nlp_pipe.fit(X_sent_train, y_sent_train)
y_sent_pred = nlp_pipe.predict(X_sent_test)
test_acc = accuracy_score(y_sent_test, y_sent_pred)
test_acc





# CodeGrade step3.4
# Replace None with appropriate code

cfm = confusion_matrix(y_sent_test, y_sent_pred)

# ConfusionMatrixDisplay(cfm).plot();
ConfusionMatrixDisplay(cfm).plot();


























# Run this cell without changes to import data
data_df = pd.read_csv('mall_clust.csv').set_index('CustomerID')
data_df.head()


# Run this cell without changes
data_df.info()


# Run this cell without changes
data_df.describe()





# CodeGrade step4.1
# Replace None with appropriate code and write additional code required to fit the data

# Scaling
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_df)

# Kmeans
km = KMeans(n_clusters=3, random_state=42)

# Fit kmeans
km.fit(data_scaled)





# CodeGrade step4.2
# Replace None with appropriate code

# Dataframe for scaled
data_scaled_df = pd.DataFrame(data_scaled, columns=data_df.columns, index=data_df.index)

# New column
data_scaled_df['cluster_label'] = km.labels_





# CodeGrade step4.3
# Replace None with appropriate code

# Create empty dictionary to populate
km_dict = {}

# Loop through k values
for k in range(3,10):
    km = KMeans(n_clusters=k, random_state=42)
    clust_pred = km.fit_predict(data_scaled)
    # For each value k get a silhouette score
    ss_metr = silhouette_score(data_scaled, clust_pred)
    # For each value of k assign a key:value pair to km_dict
    km_dict[k] = ss_metr





# Run this cell without changes

fig, ax = plt.subplots()


km_series = pd.Series(km_dict)
ax = km_series.plot()
ax.set_title('Silhouette Score for k')
ax.set_xlabel('k')
ax.set_ylabel('SS_metric')

plt.show()






